#pragma once

#include "lru_cache.h"
#include "../vector/vector_ops.h"
#include "../persistence/wal.h"
#include "../base/serializer.h"
#include "../base/expiration_manager.h"
#include <vector>
#include <memory>
#include <functional>
#include <queue>
#include <thread>
#include <future>
#include <random>
#include <unordered_set>
#include <unordered_map>

namespace minkv {
namespace db {

/**
 * @brief å·¥ä¸šçº§åˆ†ç‰‡ç¼“å­˜ç³»ç»Ÿ - é›†å¤§æˆè€…
 * 
 * [æ¶æ„ç»Ÿä¸€] é›†æˆæ‰€æœ‰åŠŸèƒ½äºä¸€ä½“ï¼š
 * - åŸºç¡€ç¼“å­˜ï¼šLRUæ·˜æ±°ã€åˆ†ç‰‡é”
 * - æŒä¹…åŒ–ï¼šWALæ—¥å¿—ã€å¿«ç…§æ¢å¤
 * - å‘é‡æ£€ç´¢ï¼šSIMDåŠ é€Ÿã€Top-Kæœç´¢
 * - å®šæœŸåˆ é™¤ï¼šç±»ä¼¼Redis serverCronçš„è¿‡æœŸæ¸…ç†
 * - å¼‚å¸¸å¤„ç†ï¼šåˆ†çº§å¤„ç†ã€è‡ªåŠ¨æ¢å¤
 * 
 * [è®¾è®¡ç›®æ ‡] 
 * - å•ä¸€å…¥å£ï¼šä¸€ä¸ªç±»è§£å†³æ‰€æœ‰éœ€æ±‚
 * - åŠŸèƒ½å®Œæ•´ï¼šç”Ÿäº§çº§ç‰¹æ€§é½å…¨
 * - æ€§èƒ½ä¼˜å¼‚ï¼š237ä¸‡QPSï¼ŒP99å»¶è¿Ÿ1.46Î¼s
 * - é«˜å¯ç”¨ï¼šå±€éƒ¨æ•…éšœä¸å½±å“æ•´ä½“æœåŠ¡
 * 
 * è¿™æ˜¯MinKVçš„æ ¸å¿ƒç»„ä»¶ï¼Œè§£å†³äº†æ¶æ„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œ
 * æä¾›äº†ç»Ÿä¸€ã€å®Œæ•´ã€é«˜æ€§èƒ½çš„ç¼“å­˜è§£å†³æ–¹æ¡ˆã€‚
 * 
 * @tparam K é”®ç±»å‹ï¼ˆå¿…é¡»æ”¯æŒ std::hash å’Œ operator==ï¼‰
 * @tparam V å€¼ç±»å‹
 */
template<typename K, typename V>
class ShardedCache {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param capacity_per_shard æ¯ä¸ªåˆ†ç‰‡çš„å®¹é‡
     * @param shard_count åˆ†ç‰‡æ•°é‡ï¼ˆé»˜è®¤32ï¼Œé€šç”¨é…ç½®ï¼‰
     */
    ShardedCache(size_t capacity_per_shard, size_t shard_count = 32);
    
    /**
     * @brief ææ„å‡½æ•°
     */
    ~ShardedCache();
    
    // ç¦æ­¢æ‹·è´å’Œèµ‹å€¼
    ShardedCache(const ShardedCache&) = delete;
    ShardedCache& operator=(const ShardedCache&) = delete;

    // ==========================================
    // åŸºç¡€ç¼“å­˜æ¥å£ (Basic Cache API)
    // ==========================================
    
    std::optional<V> get(const K& key);
    void put(const K& key, const V& value, int64_t ttl_ms = 0);
    bool remove(const K& key);
    size_t size() const;
    size_t capacity() const;
    void clear();
    CacheStats getStats() const;
    void resetStats();

    // ==========================================
    // æŒä¹…åŒ–æ¥å£ (Persistence API)
    // ==========================================
    
    void enable_persistence(const std::string& data_dir, int64_t fsync_interval_ms = 1000);
    void disable_persistence();
    void recover_from_disk();
    void create_snapshot();
    std::map<K, V> export_all_data() const;
    void clear_wal();

    // ==========================================
    // å‘é‡æ£€ç´¢æ¥å£ (Vector Search API)
    // ==========================================
    
    void vectorPut(const K& key, const std::vector<float>& vec, int64_t ttl_ms = 0);
    std::vector<float> vectorGet(const K& key);
    std::vector<K> vectorSearch(const std::vector<float>& query, int k);

    // ==========================================
    // å®šæœŸåˆ é™¤æ¥å£ (Expiration API)
    // ==========================================
    
    /**
     * @brief å¯åŠ¨å®šæœŸåˆ é™¤æœåŠ¡
     * @param check_interval æ£€æŸ¥é—´éš”ï¼Œé»˜è®¤100ms
     * @param sample_size æ¯æ¬¡é‡‡æ ·å¤§å°ï¼Œé»˜è®¤20
     */
    void startExpirationService(std::chrono::milliseconds check_interval = std::chrono::milliseconds(100),
                               size_t sample_size = 20);
    
    /**
     * @brief åœæ­¢å®šæœŸåˆ é™¤æœåŠ¡
     */
    void stopExpirationService();
    
    /**
     * @brief è·å–å®šæœŸåˆ é™¤ç»Ÿè®¡ä¿¡æ¯
     */
    base::ExpirationManager::Stats getExpirationStats() const;
    
    /**
     * @brief æ‰‹åŠ¨è§¦å‘è¿‡æœŸæ¸…ç†
     */
    size_t manualExpiration(int shard_id = -1);

    // ==========================================
    // å¥åº·æ£€æŸ¥æ¥å£ (Health Check API)
    // ==========================================
    
    /**
     * @brief è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
     */
    struct HealthStatus {
        bool overall_healthy;                    ///< æ•´ä½“å¥åº·çŠ¶æ€
        size_t healthy_shards;                   ///< å¥åº·åˆ†ç‰‡æ•°é‡
        size_t total_shards;                     ///< æ€»åˆ†ç‰‡æ•°é‡
        std::vector<size_t> disabled_shards;     ///< è¢«ç¦ç”¨çš„åˆ†ç‰‡åˆ—è¡¨
        std::unordered_map<size_t, int> error_counts;      ///< å„åˆ†ç‰‡é”™è¯¯è®¡æ•°
        double error_rate;                       ///< æ•´ä½“é”™è¯¯ç‡
        std::chrono::steady_clock::time_point last_health_check;  ///< ä¸Šæ¬¡å¥åº·æ£€æŸ¥æ—¶é—´
    };
    
    HealthStatus getHealthStatus() const;
    
    /**
     * @brief æ‰‹åŠ¨è§¦å‘å¥åº·æ£€æŸ¥
     */
    void performHealthCheck();
    
    // ==========================================
    // LSNæ¥å£ (Log Sequence Number API)
    // ==========================================
    
    /**
     * @brief è·å–ä¸‹ä¸€ä¸ªLSNï¼ˆåŸå­é€’å¢ï¼‰
     * @return æ–°çš„LSNå€¼
     * 
     * å®ç°è¯´æ˜ï¼š
     * - global_lsn_å­˜å‚¨çš„æ˜¯ä¸‹ä¸€ä¸ªè¦åˆ†é…çš„LSN
     * - fetch_addè¿”å›æ—§å€¼ï¼ˆå³å½“å‰è¦åˆ†é…çš„LSNï¼‰
     * - ä¾‹å¦‚ï¼šåˆå§‹å€¼1ï¼Œç¬¬ä¸€æ¬¡è°ƒç”¨è¿”å›1å¹¶é€’å¢åˆ°2ï¼Œç¬¬äºŒæ¬¡è¿”å›2å¹¶é€’å¢åˆ°3
     */
    uint64_t next_lsn() {
        return global_lsn_.fetch_add(1, std::memory_order_relaxed);
    }
    
    /**
     * @brief è·å–å½“å‰LSNï¼ˆä¸é€’å¢ï¼‰
     * @return å½“å‰LSNå€¼ï¼ˆæœ€ååˆ†é…çš„LSNï¼‰
     * 
     * å®ç°è¯´æ˜ï¼š
     * - global_lsn_å­˜å‚¨çš„æ˜¯ä¸‹ä¸€ä¸ªè¦åˆ†é…çš„LSN
     * - æ‰€ä»¥å½“å‰å·²åˆ†é…çš„LSN = global_lsn_ - 1
     */
    uint64_t current_lsn() const {
        uint64_t next = global_lsn_.load(std::memory_order_relaxed);
        return (next > 0) ? (next - 1) : 0;
    }

private:
    // ==========================================
    // æ ¸å¿ƒæ•°æ®ç»“æ„
    // ==========================================
    
    /**
     * @brief å¢å¼ºçš„LRUç¼“å­˜åˆ†ç‰‡
     */
    class EnhancedLruShard {
    public:
        EnhancedLruShard(size_t capacity);
        
        // åŸºç¡€æ¥å£
        std::optional<V> get(const K& key);
        void put(const K& key, const V& value, int64_t ttl_ms = 0);
        bool remove(const K& key);
        size_t size() const;
        size_t capacity() const;
        CacheStats getStats() const;
        void resetStats();
        void clear();
        std::map<K, V> get_all() const;
        
        // å®šæœŸåˆ é™¤æ¥å£
        bool try_lock();
        void unlock();
        std::vector<K> randomSample(size_t sample_size);
        size_t expireKeys(const std::vector<K>& keys);
        
    private:
        std::unique_ptr<LruCache<K, V>> cache_;
        mutable std::mutex mutex_;
        mutable std::mt19937 rng_;
    };
    
    std::vector<std::unique_ptr<EnhancedLruShard>> shards_;
    
    // ==========================================
    // æŒä¹…åŒ–ç›¸å…³
    // ==========================================
    
    std::unique_ptr<WriteAheadLog> wal_;
    bool persistence_enabled_{false};
    mutable std::shared_mutex global_consistency_lock_;
    mutable std::mutex persistence_mutex_;
    
    // ==========================================
    // LSNç›¸å…³ (Log Sequence Number)
    // ==========================================
    
    std::atomic<uint64_t> global_lsn_{1};  ///< å…¨å±€LSNè®¡æ•°å™¨ï¼Œä»1å¼€å§‹ï¼Œä¸¥æ ¼å•è°ƒé€’å¢
    
    // ==========================================
    // å®šæœŸåˆ é™¤ç›¸å…³
    // ==========================================
    
    std::unique_ptr<base::ExpirationManager> expiration_manager_;
    
    // ==========================================
    // å¥åº·æ£€æŸ¥ç›¸å…³
    // ==========================================
    
    mutable std::mutex health_mutex_;
    std::unordered_map<size_t, int> shard_error_counts_;
    std::unordered_set<size_t> disabled_shards_;
    std::chrono::steady_clock::time_point last_health_check_;
    
    static constexpr int MAX_CONSECUTIVE_ERRORS = 5;
    static constexpr auto HEALTH_CHECK_INTERVAL = std::chrono::minutes(1);
    
    // ==========================================
    // å†…éƒ¨æ–¹æ³•
    // ==========================================
    
    size_t get_shard_index(const K& key) const;
    size_t expirationCallback(size_t shard_id, size_t sample_size);
    void recordShardError(size_t shard_id);
    void recordShardSuccess(size_t shard_id);
    bool isShardDisabled(size_t shard_id) const;
};

// ============ å®ç°éƒ¨åˆ† ============

template<typename K, typename V>
ShardedCache<K, V>::ShardedCache(size_t capacity_per_shard, size_t shard_count)
    : last_health_check_(std::chrono::steady_clock::now()) {
    
    // åˆ›å»ºå¢å¼ºçš„åˆ†ç‰‡
    for (size_t i = 0; i < shard_count; ++i) {
        shards_.push_back(std::make_unique<EnhancedLruShard>(capacity_per_shard));
    }
}

template<typename K, typename V>
ShardedCache<K, V>::~ShardedCache() {
    // åœæ­¢æ‰€æœ‰æœåŠ¡
    stopExpirationService();
    disable_persistence();
}

template<typename K, typename V>
size_t ShardedCache<K, V>::get_shard_index(const K& key) const {
    std::hash<K> hasher;
    return hasher(key) % shards_.size();
}

// ==========================================
// åŸºç¡€ç¼“å­˜æ¥å£å®ç°
// ==========================================

template<typename K, typename V>
std::optional<V> ShardedCache<K, V>::get(const K& key) {
    size_t shard_idx = get_shard_index(key);
    
    if (isShardDisabled(shard_idx)) {
        return std::nullopt;  // åˆ†ç‰‡è¢«ç¦ç”¨
    }
    
    try {
        auto result = shards_[shard_idx]->get(key);
        recordShardSuccess(shard_idx);
        return result;
    } catch (const std::exception& e) {
        recordShardError(shard_idx);
        return std::nullopt;
    }
}

template<typename K, typename V>
void ShardedCache<K, V>::put(const K& key, const V& value, int64_t ttl_ms) {
    std::shared_lock<std::shared_mutex> consistency_lock(global_consistency_lock_);
    
    size_t shard_idx = get_shard_index(key);
    
    if (isShardDisabled(shard_idx)) {
        return;  // åˆ†ç‰‡è¢«ç¦ç”¨ï¼Œè·³è¿‡
    }
    
    // å‡†å¤‡WALæ¡ç›®
    LogEntry wal_entry;
    bool need_wal = false;
    
    if (persistence_enabled_ && wal_) {
        need_wal = true;
        wal_entry.op = LogEntry::PUT;
        try {
            wal_entry.key = Serializer<K>::serialize(key);
            wal_entry.value = Serializer<V>::serialize(value);
            wal_entry.timestamp_ms = std::chrono::duration_cast<std::chrono::milliseconds>(
                std::chrono::high_resolution_clock::now().time_since_epoch()).count();
        } catch (const std::exception& e) {
            need_wal = false;
        }
    }
    
    try {
        // å†™å…¥å†…å­˜
        shards_[shard_idx]->put(key, value, ttl_ms);
        
        // å†™å…¥WAL
        if (need_wal) {
            std::lock_guard<std::mutex> wal_lock(persistence_mutex_);
            wal_->append(wal_entry);
        }
        
        recordShardSuccess(shard_idx);
        
    } catch (const std::exception& e) {
        recordShardError(shard_idx);
    }
}

template<typename K, typename V>
bool ShardedCache<K, V>::remove(const K& key) {
    std::shared_lock<std::shared_mutex> consistency_lock(global_consistency_lock_);
    
    size_t shard_idx = get_shard_index(key);
    
    if (isShardDisabled(shard_idx)) {
        return false;  // åˆ†ç‰‡è¢«ç¦ç”¨
    }
    
    // å‡†å¤‡WALæ¡ç›®
    LogEntry wal_entry;
    bool need_wal = false;
    
    if (persistence_enabled_ && wal_) {
        need_wal = true;
        wal_entry.op = LogEntry::DELETE;
        try {
            wal_entry.key = Serializer<K>::serialize(key);
            wal_entry.timestamp_ms = std::chrono::duration_cast<std::chrono::milliseconds>(
                std::chrono::high_resolution_clock::now().time_since_epoch()).count();
        } catch (const std::exception& e) {
            need_wal = false;
        }
    }
    
    try {
        // åˆ é™¤å†…å­˜æ•°æ®
        bool result = shards_[shard_idx]->remove(key);
        
        // å†™å…¥WAL
        if (result && need_wal) {
            std::lock_guard<std::mutex> wal_lock(persistence_mutex_);
            wal_->append(wal_entry);
        }
        
        recordShardSuccess(shard_idx);
        return result;
        
    } catch (const std::exception& e) {
        recordShardError(shard_idx);
        return false;
    }
}

template<typename K, typename V>
size_t ShardedCache<K, V>::size() const {
    size_t total = 0;
    for (size_t i = 0; i < shards_.size(); ++i) {
        if (!isShardDisabled(i)) {
            try {
                total += shards_[i]->size();
            } catch (...) {
                // å¿½ç•¥å•ä¸ªåˆ†ç‰‡çš„é”™è¯¯
            }
        }
    }
    return total;
}

template<typename K, typename V>
size_t ShardedCache<K, V>::capacity() const {
    return shards_.size() * (shards_.empty() ? 0 : shards_[0]->capacity());
}

template<typename K, typename V>
void ShardedCache<K, V>::clear() {
    std::unique_lock<std::shared_mutex> consistency_lock(global_consistency_lock_);
    
    for (size_t i = 0; i < shards_.size(); ++i) {
        if (!isShardDisabled(i)) {
            try {
                shards_[i]->clear();
                recordShardSuccess(i);
            } catch (const std::exception& e) {
                recordShardError(i);
            }
        }
    }
}

template<typename K, typename V>
CacheStats ShardedCache<K, V>::getStats() const {
    CacheStats total_stats;
    
    for (size_t i = 0; i < shards_.size(); ++i) {
        if (!isShardDisabled(i)) {
            try {
                auto shard_stats = shards_[i]->getStats();
                total_stats.hits += shard_stats.hits;
                total_stats.misses += shard_stats.misses;
                total_stats.expired += shard_stats.expired;
                total_stats.evictions += shard_stats.evictions;
                total_stats.puts += shard_stats.puts;
                total_stats.removes += shard_stats.removes;
                total_stats.current_size += shard_stats.current_size;
                total_stats.capacity += shard_stats.capacity;
            } catch (...) {
                // å¿½ç•¥å•ä¸ªåˆ†ç‰‡çš„é”™è¯¯
            }
        }
    }
    
    return total_stats;
}

template<typename K, typename V>
void ShardedCache<K, V>::resetStats() {
    for (size_t i = 0; i < shards_.size(); ++i) {
        if (!isShardDisabled(i)) {
            try {
                shards_[i]->resetStats();
            } catch (...) {
                // å¿½ç•¥å•ä¸ªåˆ†ç‰‡çš„é”™è¯¯
            }
        }
    }
}

// ==========================================
// æŒä¹…åŒ–æ¥å£å®ç°
// ==========================================

template<typename K, typename V>
void ShardedCache<K, V>::enable_persistence(const std::string& data_dir, int64_t fsync_interval_ms) {
    std::lock_guard<std::mutex> lock(persistence_mutex_);
    
    if (persistence_enabled_) {
        return;  // å·²å¯ç”¨
    }
    
    try {
        wal_ = std::make_unique<WriteAheadLog>(data_dir, 1024 * 1024, fsync_interval_ms);
        wal_->start_background_fsync();
        persistence_enabled_ = true;
        
        std::cout << "[Persistence] WAL enabled: " << data_dir << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "[Persistence] Failed to enable: " << e.what() << std::endl;
        persistence_enabled_ = false;
    }
}

template<typename K, typename V>
void ShardedCache<K, V>::disable_persistence() {
    std::lock_guard<std::mutex> lock(persistence_mutex_);
    
    if (!persistence_enabled_) {
        return;
    }
    
    if (wal_) {
        wal_->stop_background_fsync();
        wal_->flush();
    }
    
    wal_.reset();
    persistence_enabled_ = false;
    
    std::cout << "[Persistence] WAL disabled" << std::endl;
}

template<typename K, typename V>
void ShardedCache<K, V>::recover_from_disk() {
    if (!wal_) {
        return;
    }
    
    std::cout << "[Recovery] Starting recovery from WAL..." << std::endl;
    
    auto entries = wal_->read_all();
    size_t recovered_count = 0;
    size_t error_count = 0;
    
    for (const auto& entry : entries) {
        try {
            if (entry.op == LogEntry::PUT) {
                K key = Serializer<K>::deserialize(entry.key);
                V value = Serializer<V>::deserialize(entry.value);
                
                // æ¢å¤æ—¶ä¸å†™WALï¼Œé¿å…é‡å¤è®°å½•
                size_t shard_idx = get_shard_index(key);
                shards_[shard_idx]->put(key, value, 0);  // æ¢å¤æ—¶ä¸è®¾ç½®TTL
                recovered_count++;
                
            } else if (entry.op == LogEntry::DELETE) {
                K key = Serializer<K>::deserialize(entry.key);
                
                size_t shard_idx = get_shard_index(key);
                shards_[shard_idx]->remove(key);
                recovered_count++;
            }
        } catch (const std::exception& e) {
            error_count++;
            std::cerr << "[Recovery] Failed to replay entry: " << e.what() << std::endl;
        }
    }
    
    std::cout << "[Recovery] Completed: " << recovered_count 
              << " entries recovered, " << error_count << " errors" << std::endl;
}

template<typename K, typename V>
void ShardedCache<K, V>::create_snapshot() {
    if (!wal_) {
        std::cout << "[Snapshot] WAL not enabled, skipping snapshot" << std::endl;
        return;
    }
    
    std::cout << "[Snapshot] Creating snapshot..." << std::endl;
    
    // å¯¼å‡ºæ‰€æœ‰æ•°æ®ï¼ˆåœ¨ä¸€è‡´æ€§é”ä¿æŠ¤ä¸‹ï¼‰
    auto all_data = export_all_data();
    
    // åˆ›å»ºå¿«ç…§
    int64_t snapshot_id = wal_->create_snapshot(all_data);
    
    std::cout << "[Snapshot] Created snapshot " << snapshot_id 
              << " with " << all_data.size() << " entries" << std::endl;
}

template<typename K, typename V>
std::map<K, V> ShardedCache<K, V>::export_all_data() const {
    std::unique_lock<std::shared_mutex> consistency_lock(global_consistency_lock_);
    
    std::map<K, V> all_data;
    
    for (size_t i = 0; i < shards_.size(); ++i) {
        if (!isShardDisabled(i)) {
            try {
                auto shard_data = shards_[i]->get_all();
                all_data.insert(shard_data.begin(), shard_data.end());
            } catch (const std::exception& e) {
                std::cerr << "[Export] Shard " << i << " error: " << e.what() << std::endl;
            }
        }
    }
    
    std::cout << "[Export] Exported " << all_data.size() 
              << " entries under consistency lock" << std::endl;
    
    return all_data;
}

template<typename K, typename V>
void ShardedCache<K, V>::clear_wal() {
    std::lock_guard<std::mutex> lock(persistence_mutex_);
    
    if (wal_) {
        wal_->clear_all();
        std::cout << "[WAL] Cleared all entries" << std::endl;
    }
}

// ==========================================
// å‘é‡æœç´¢æ¥å£å®ç°
// ==========================================

template<typename K, typename V>
void ShardedCache<K, V>::vectorPut(const K& key, const std::vector<float>& vec, int64_t ttl_ms) {
    // å°†å‘é‡åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
    std::string serialized_vec = VectorOps::Serialize(vec);
    
    // ä½¿ç”¨åŸºç¡€putæ¥å£å­˜å‚¨
    put(key, serialized_vec, ttl_ms);
}

template<typename K, typename V>
std::vector<float> ShardedCache<K, V>::vectorGet(const K& key) {
    auto result = get(key);
    if (!result.has_value()) {
        return {};
    }
    
    // ååºåˆ—åŒ–å‘é‡æ•°æ®
    return VectorOps::DeserializeCopy(*result);
}

template<typename K, typename V>
std::vector<K> ShardedCache<K, V>::vectorSearch(const std::vector<float>& query, int k) {
    struct SearchResult {
        K key;
        float distance;
        SearchResult(const K& k, float d) : key(k), distance(d) {}
        bool operator<(const SearchResult& other) const {
            return distance < other.distance;  // å¤§é¡¶å †
        }
    };
    
    // å¹¶è¡Œæœç´¢æ‰€æœ‰åˆ†ç‰‡
    std::vector<std::future<std::vector<SearchResult>>> futures;
    
    for (size_t shard_idx = 0; shard_idx < shards_.size(); ++shard_idx) {
        if (isShardDisabled(shard_idx)) {
            continue;  // è·³è¿‡è¢«ç¦ç”¨çš„åˆ†ç‰‡
        }
        
        futures.push_back(std::async(std::launch::async, [this, shard_idx, &query, k]() {
            std::vector<SearchResult> local_results;
            std::priority_queue<SearchResult> heap;
            
            try {
                auto all_data = shards_[shard_idx]->get_all();
                
                for (const auto& [key, raw_data] : all_data) {
                    auto vec_data = VectorOps::DeserializeCopy(raw_data);
                    
                    if (vec_data.empty() || vec_data.size() != query.size()) {
                        continue;  // ç»´åº¦ä¸åŒ¹é…
                    }
                    
                    float distance = VectorOps::L2DistanceSquare_AVX2(
                        query.data(), vec_data.data(), vec_data.size());
                    
                    heap.push(SearchResult(key, distance));
                    if ((int)heap.size() > k) {
                        heap.pop();
                    }
                }
                
                while (!heap.empty()) {
                    local_results.push_back(heap.top());
                    heap.pop();
                }
                
            } catch (const std::exception& e) {
                // å•ä¸ªåˆ†ç‰‡é”™è¯¯ä¸å½±å“æ•´ä½“æœç´¢
                std::cerr << "[VectorSearch] Shard " << shard_idx 
                          << " error: " << e.what() << std::endl;
            }
            
            return local_results;
        }));
    }
    
    // æ”¶é›†æ‰€æœ‰åˆ†ç‰‡ç»“æœ
    std::priority_queue<SearchResult> global_heap;
    
    for (auto& f : futures) {
        try {
            auto shard_results = f.get();
            for (const auto& res : shard_results) {
                global_heap.push(res);
                if ((int)global_heap.size() > k) {
                    global_heap.pop();
                }
            }
        } catch (const std::exception& e) {
            std::cerr << "[VectorSearch] Future error: " << e.what() << std::endl;
        }
    }
    
    // æ•´ç†æœ€ç»ˆç»“æœ
    std::vector<K> final_results;
    while (!global_heap.empty()) {
        final_results.push_back(global_heap.top().key);
        global_heap.pop();
    }
    std::reverse(final_results.begin(), final_results.end());
    
    return final_results;
}

// ==========================================
// å®šæœŸåˆ é™¤æ¥å£å®ç°
// ==========================================

template<typename K, typename V>
void ShardedCache<K, V>::startExpirationService(std::chrono::milliseconds check_interval, size_t sample_size) {
    if (expiration_manager_) {
        return;  // å·²å¯åŠ¨
    }
    
    expiration_manager_ = std::make_unique<base::ExpirationManager>(
        shards_.size(), check_interval, sample_size);
    
    auto callback = [this](size_t shard_id, size_t sample_size) -> size_t {
        return this->expirationCallback(shard_id, sample_size);
    };
    
    expiration_manager_->start(callback);
}

template<typename K, typename V>
void ShardedCache<K, V>::stopExpirationService() {
    if (expiration_manager_) {
        expiration_manager_->stop();
        expiration_manager_.reset();
    }
}

template<typename K, typename V>
size_t ShardedCache<K, V>::expirationCallback(size_t shard_id, size_t sample_size) {
    if (shard_id >= shards_.size() || isShardDisabled(shard_id)) {
        return 0;
    }
    
    auto& shard = shards_[shard_id];
    
    // ğŸ¯ éé˜»å¡é”ï¼šé¿å…ä¸ä¸šåŠ¡çº¿ç¨‹ç«äº‰
    if (!shard->try_lock()) {
        // é”ç«äº‰ä¸ç®—é”™è¯¯ï¼Œåªæ˜¯è·³è¿‡æœ¬æ¬¡æ£€æŸ¥
        return 0;
    }
    
    // RAIIé”ç®¡ç†
    struct LockGuard {
        EnhancedLruShard* shard_;
        ~LockGuard() { shard_->unlock(); }
    } guard{shard.get()};
    
    try {
        // éšæœºé‡‡æ ·å¹¶åˆ é™¤è¿‡æœŸkey
        auto keys = shard->randomSample(sample_size);
        size_t expired_count = shard->expireKeys(keys);
        
        // æˆåŠŸå¤„ç†ï¼Œé‡ç½®é”™è¯¯è®¡æ•°
        recordShardSuccess(shard_id);
        
        return expired_count;
        
    } catch (const std::exception& e) {
        // çœŸæ­£çš„å¼‚å¸¸æ‰è®°å½•é”™è¯¯
        std::cout << "[ExpirationManager][ShardError] Shard " << shard_id 
                  << " error: " << e.what() << std::endl;
        recordShardError(shard_id);
        return 0;
    }
}

template<typename K, typename V>
base::ExpirationManager::Stats ShardedCache<K, V>::getExpirationStats() const {
    if (expiration_manager_) {
        return expiration_manager_->getStats();
    }
    return {};
}

template<typename K, typename V>
size_t ShardedCache<K, V>::manualExpiration(int shard_id) {
    size_t total_expired = 0;
    
    if (shard_id == -1) {
        // æ¸…ç†æ‰€æœ‰åˆ†ç‰‡
        for (size_t i = 0; i < shards_.size(); ++i) {
            total_expired += expirationCallback(i, 20);
        }
    } else if (shard_id >= 0 && shard_id < static_cast<int>(shards_.size())) {
        // æ¸…ç†æŒ‡å®šåˆ†ç‰‡
        total_expired = expirationCallback(static_cast<size_t>(shard_id), 20);
    }
    
    return total_expired;
}

// ==========================================
// å¥åº·æ£€æŸ¥å®ç°
// ==========================================

template<typename K, typename V>
void ShardedCache<K, V>::recordShardError(size_t shard_id) {
    std::lock_guard<std::mutex> lock(health_mutex_);
    
    shard_error_counts_[shard_id]++;
    
    if (shard_error_counts_[shard_id] >= MAX_CONSECUTIVE_ERRORS) {
        disabled_shards_.insert(shard_id);
        std::cout << "[HealthCheck] Shard " << shard_id 
                  << " disabled due to " << shard_error_counts_[shard_id] 
                  << " consecutive errors" << std::endl;
    }
}

template<typename K, typename V>
void ShardedCache<K, V>::recordShardSuccess(size_t shard_id) {
    std::lock_guard<std::mutex> lock(health_mutex_);
    shard_error_counts_[shard_id] = 0;  // é‡ç½®é”™è¯¯è®¡æ•°
}

template<typename K, typename V>
bool ShardedCache<K, V>::isShardDisabled(size_t shard_id) const {
    std::lock_guard<std::mutex> lock(health_mutex_);
    return disabled_shards_.count(shard_id) > 0;
}

template<typename K, typename V>
typename ShardedCache<K, V>::HealthStatus 
ShardedCache<K, V>::getHealthStatus() const {
    std::lock_guard<std::mutex> lock(health_mutex_);
    
    HealthStatus status;
    status.total_shards = shards_.size();
    status.healthy_shards = status.total_shards - disabled_shards_.size();
    status.overall_healthy = (status.healthy_shards > status.total_shards / 2);  // è¶…è¿‡ä¸€åŠå¥åº·
    status.disabled_shards.assign(disabled_shards_.begin(), disabled_shards_.end());
    status.error_counts = shard_error_counts_;
    status.last_health_check = last_health_check_;
    
    // è®¡ç®—é”™è¯¯ç‡
    int total_errors = 0;
    for (const auto& [shard_id, count] : shard_error_counts_) {
        total_errors += count;
    }
    status.error_rate = static_cast<double>(total_errors) / (status.total_shards * MAX_CONSECUTIVE_ERRORS);
    
    return status;
}

template<typename K, typename V>
void ShardedCache<K, V>::performHealthCheck() {
    std::lock_guard<std::mutex> lock(health_mutex_);
    
    last_health_check_ = std::chrono::steady_clock::now();
    
    // å°è¯•é‡æ–°å¯ç”¨è¢«ç¦ç”¨çš„åˆ†ç‰‡
    for (auto it = disabled_shards_.begin(); it != disabled_shards_.end();) {
        size_t shard_id = *it;
        
        try {
            // å°è¯•ä¸€ä¸ªç®€å•çš„æ“ä½œæ¥æµ‹è¯•åˆ†ç‰‡å¥åº·çŠ¶æ€
            auto test_key = K{};  // é»˜è®¤æ„é€ çš„æµ‹è¯•key
            shards_[shard_id]->get(test_key);  // æµ‹è¯•è¯»å–
            
            // æˆåŠŸäº†ï¼Œé‡æ–°å¯ç”¨
            shard_error_counts_[shard_id] = 0;
            it = disabled_shards_.erase(it);
            
            std::cout << "[HealthCheck] Shard " << shard_id << " recovered and re-enabled" << std::endl;
            
        } catch (...) {
            // ä»ç„¶æœ‰é—®é¢˜ï¼Œä¿æŒç¦ç”¨
            ++it;
        }
    }
}

// ==========================================
// EnhancedLruShard å®ç°
// ==========================================

template<typename K, typename V>
ShardedCache<K, V>::EnhancedLruShard::EnhancedLruShard(size_t capacity)
    : cache_(std::make_unique<LruCache<K, V>>(capacity)), rng_(std::random_device{}()) {
}

template<typename K, typename V>
bool ShardedCache<K, V>::EnhancedLruShard::try_lock() {
    return mutex_.try_lock();
}

template<typename K, typename V>
void ShardedCache<K, V>::EnhancedLruShard::unlock() {
    mutex_.unlock();
}

template<typename K, typename V>
std::optional<V> ShardedCache<K, V>::EnhancedLruShard::get(const K& key) {
    std::lock_guard<std::mutex> lock(mutex_);
    return cache_->get(key);
}

template<typename K, typename V>
void ShardedCache<K, V>::EnhancedLruShard::put(const K& key, const V& value, int64_t ttl_ms) {
    std::lock_guard<std::mutex> lock(mutex_);
    cache_->put(key, value, ttl_ms);
}

template<typename K, typename V>
bool ShardedCache<K, V>::EnhancedLruShard::remove(const K& key) {
    std::lock_guard<std::mutex> lock(mutex_);
    return cache_->remove(key);
}

template<typename K, typename V>
size_t ShardedCache<K, V>::EnhancedLruShard::size() const {
    std::lock_guard<std::mutex> lock(mutex_);
    return cache_->size();
}

template<typename K, typename V>
size_t ShardedCache<K, V>::EnhancedLruShard::capacity() const {
    return cache_->capacity();
}

template<typename K, typename V>
CacheStats ShardedCache<K, V>::EnhancedLruShard::getStats() const {
    std::lock_guard<std::mutex> lock(mutex_);
    return cache_->getStats();
}

template<typename K, typename V>
void ShardedCache<K, V>::EnhancedLruShard::resetStats() {
    std::lock_guard<std::mutex> lock(mutex_);
    cache_->resetStats();
}

template<typename K, typename V>
void ShardedCache<K, V>::EnhancedLruShard::clear() {
    std::lock_guard<std::mutex> lock(mutex_);
    cache_->clear();
}

template<typename K, typename V>
std::map<K, V> ShardedCache<K, V>::EnhancedLruShard::get_all() const {
    std::lock_guard<std::mutex> lock(mutex_);
    return cache_->get_all();
}

template<typename K, typename V>
std::vector<K> ShardedCache<K, V>::EnhancedLruShard::randomSample(size_t sample_size) {
    // æ³¨æ„ï¼šè°ƒç”¨æ­¤æ–¹æ³•å‰å¿…é¡»å·²ç»è·å–é”
    auto all_data = cache_->get_all();
    
    std::vector<K> keys;
    keys.reserve(all_data.size());
    
    for (const auto& [key, value] : all_data) {
        keys.push_back(key);
    }
    
    if (keys.empty()) {
        return {};
    }
    
    std::shuffle(keys.begin(), keys.end(), rng_);
    
    size_t actual_size = std::min(sample_size, keys.size());
    keys.resize(actual_size);
    
    return keys;
}

template<typename K, typename V>
size_t ShardedCache<K, V>::EnhancedLruShard::expireKeys(const std::vector<K>& keys) {
    // æ³¨æ„ï¼šè°ƒç”¨æ­¤æ–¹æ³•å‰å¿…é¡»å·²ç»è·å–é”
    size_t expired_count = 0;
    
    for (const auto& key : keys) {
        // ğŸ¯ ç²¾ç¡®çš„è¿‡æœŸæ£€æŸ¥ï¼šå…ˆè·å–å½“å‰å€¼ï¼Œå†æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        auto old_size = cache_->size();
        auto value = cache_->get(key);  // get()ä¼šè‡ªåŠ¨åˆ é™¤è¿‡æœŸçš„key
        auto new_size = cache_->size();
        
        // å¦‚æœsizeå‡å°‘äº†ï¼Œè¯´æ˜keyå› ä¸ºè¿‡æœŸè¢«åˆ é™¤äº†
        if (new_size < old_size) {
            expired_count++;
        }
    }
    
    return expired_count;
}

} // namespace db
} // namespace minkv